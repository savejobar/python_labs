{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "820eeb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from http import HTTPStatus\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a9e04b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Загружает HTML-страницу по URL и возвращает её текстовое содержимое.\n",
    "\n",
    "    Args:\n",
    "        url (str): Ссылка на веб-страницу.\n",
    "\n",
    "    Returns:\n",
    "        str: Текстовое содержимое страницы.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != HTTPStatus.OK:\n",
    "        raise Exception(f'Invalid http status. got {response.status_code}')\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "be363ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_w(filename: str, content: str) -> None:\n",
    "    \"\"\"\n",
    "    Записывает строку в файл с кодировкой UTF-8.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Имя файла.\n",
    "        content (str): Содержимое для записи.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "521314af",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_url = 'https://www.ultralytics.com/ru/glossary/convolutional-neural-network-cnn'\n",
    "file_w('file_text.txt', get_page(page_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d3d94928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seek_of_names(text_of_file: str) -> str:\n",
    "    \"\"\"\n",
    "    Извлекает из текста все слова, начинающиеся с заглавной буквы.\n",
    "    Возвращает строку слов, разделённых '; '.\n",
    "\n",
    "    Args:\n",
    "        text_of_file (str): Текст для обработки.\n",
    "\n",
    "    Returns:\n",
    "        str: Слова, начинающиеся с заглавной буквы, через '; '.\n",
    "    \"\"\"\n",
    "    if not isinstance(text_of_file, str):\n",
    "        raise ValueError('file must be a string')\n",
    "    text_of_file = re.sub(r\"<.*?>\", \" \", text_of_file)\n",
    "    text_of_file = re.sub(r\"\\s+\", \" \", text_of_file)\n",
    "    words = re.findall(r'\\b[A-ZА-ЯЁ][a-zа-яёA-ZА-ЯЁ]+\\b', text_of_file)\n",
    "    result = \"; \".join(words)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7eb508de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN; Ultralytics; Theia; Scientific; Ultralytics; YOLO; Docs; GitHub; YOLO; Vision; Ultralytics; Insights; Ultralytics; Ultralytics; Ultralytics; GitHub; GitHub; YOLO; Vision; Ultralytics; CNN; CNN; Ultralytics; YOLO; Ultralytics; CNN; CNN; CNN; ReLU; Pooling; Downsampling; ImageNet; Classification; Deep; Convolutional; Neural; Networks; Cnn; CNN; NN; CNN; ViTs; CNN; ViTs; ViT; RT; CNN; Ultralytics; YOLO; CNN; CNN; NIH; Ultralytics; CNN; Net; CNN; PyTorch; PyTorch; TensorFlow; API; CNN; API; Keras; Ultralytics; HUB; OpenVINO; TensorRT; Ultralytics; YOLO; Vision; Vision; Judicial; MD; Cra; Centro; Madrid; CR; Land; Tower; Kefa; Road; Ultralytics; WeChat; YOLOUltralytics; YOLO; Ultralytics; Ultralytics; Inc\n"
     ]
    }
   ],
   "source": [
    "with open ('file_text.txt', encoding = 'utf-8') as f:\n",
    "    text_of_file = f.read()\n",
    "print(seek_of_names(text_of_file))\n",
    "\n",
    "file_w('file_test_of_names.txt', seek_of_names(text_of_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4a17f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursion_search(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Рекурсивно ищет самое длинное слово в файле.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Имя файла.\n",
    "\n",
    "    Returns:\n",
    "        str: Самое длинное слово в файле.\n",
    "    \"\"\"\n",
    "    def helper(words, i=0, longest=\"\"):\n",
    "        if i == len(words):\n",
    "            return longest\n",
    "\n",
    "        if len(words[i]) > len(longest):\n",
    "            longest = words[i]\n",
    "\n",
    "        return helper(words, i + 1, longest)\n",
    "\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    words = text.replace(';', '').split()\n",
    "\n",
    "    return helper(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "855d30ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YOLOUltralytics'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recursion_search('file_test_of_names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c808776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ununique_names(filename: str) -> dict:\n",
    "    \"\"\"\n",
    "    Подсчитывает количество повторений каждого слова, начинающегося с заглавной буквы.\n",
    "\n",
    "    Args:\n",
    "        text_of_file (str): Текст для анализа.\n",
    "\n",
    "    Returns:\n",
    "        dict: Словарь {слово: количество}.\n",
    "    \"\"\"\n",
    "    if not isinstance(filename, str):\n",
    "        raise ValueError('file must be a string')\n",
    "    with open (filename, encoding = 'utf-8') as f:\n",
    "        text_of_file = f.read()\n",
    "    text_of_file = re.sub(r\"<.*?>\", \" \", text_of_file)\n",
    "    text_of_file = re.sub(r\"\\s+\", \" \", text_of_file)\n",
    "    words = re.findall(r'\\b[A-ZА-ЯЁ][a-zа-яёA-ZА-ЯЁ]*\\b', text_of_file)\n",
    "    counts = reduce(lambda acc, w: {**acc, w: acc.get(w, 0)+1}, words, {})\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b20c2efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CNN': 15,\n",
       " 'Ultralytics': 16,\n",
       " 'Theia': 1,\n",
       " 'Scientific': 1,\n",
       " 'YOLO': 7,\n",
       " 'Docs': 1,\n",
       " 'GitHub': 3,\n",
       " 'Vision': 4,\n",
       " 'Insights': 1,\n",
       " 'ReLU': 1,\n",
       " 'Pooling': 1,\n",
       " 'Downsampling': 1,\n",
       " 'ImageNet': 1,\n",
       " 'Classification': 1,\n",
       " 'Deep': 1,\n",
       " 'Convolutional': 1,\n",
       " 'Neural': 1,\n",
       " 'Networks': 1,\n",
       " 'Cnn': 1,\n",
       " 'NN': 1,\n",
       " 'ViTs': 2,\n",
       " 'ViT': 1,\n",
       " 'RT': 1,\n",
       " 'NIH': 1,\n",
       " 'Net': 1,\n",
       " 'PyTorch': 2,\n",
       " 'TensorFlow': 1,\n",
       " 'API': 2,\n",
       " 'Keras': 1,\n",
       " 'HUB': 1,\n",
       " 'OpenVINO': 1,\n",
       " 'TensorRT': 1,\n",
       " 'Judicial': 1,\n",
       " 'MD': 1,\n",
       " 'Cra': 1,\n",
       " 'Centro': 1,\n",
       " 'Madrid': 1,\n",
       " 'CR': 1,\n",
       " 'Land': 1,\n",
       " 'Tower': 1,\n",
       " 'Kefa': 1,\n",
       " 'Road': 1,\n",
       " 'WeChat': 1,\n",
       " 'YOLOUltralytics': 1,\n",
       " 'Inc': 1}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ununique_names('file_test_of_names.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b7dcf1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_names(dictionary: dict, N: int) -> None:\n",
    "    \"\"\"\n",
    "    Выводит N самых часто встречающихся слов.\n",
    "\n",
    "    Args:\n",
    "        dictionary (dict): Словарь {слово: количество}.\n",
    "        N (int): Сколько топ-слов выводить.\n",
    "    \"\"\"\n",
    "    dictionary = sorted(dictionary.items(), reverse = True, key = lambda item: item[1])[:N]\n",
    "    for key, value in dictionary:\n",
    "        print(f'{key} = {value}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8666d6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics = 16\n"
     ]
    }
   ],
   "source": [
    "count_names(ununique_names('file_test_of_names.txt'), 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
